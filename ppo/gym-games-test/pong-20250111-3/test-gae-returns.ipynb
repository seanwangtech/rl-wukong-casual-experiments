{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "# seans agent\n",
    "class ppo_agent():\n",
    "    def __init__(self, gamma=0.99, lam=0.95, clip_epsilon=0.2, device='cpu', norm_adv=True):\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam  # Lambda for GAE\n",
    "        self.clip_epsilon = clip_epsilon  # Clipping range for policy loss\n",
    "        self.device = device\n",
    "        self.norm_adv = norm_adv\n",
    "        \n",
    "    def compute_gae(self, rewards, values, dones):\n",
    "        values = list(values) + [0]  # Convert values to a list before concatenation\n",
    "        advantages = []\n",
    "        gae = 0\n",
    "        for step in reversed(range(len(rewards))):\n",
    "            # print( rewards[step], values[step + 1] , 1 - dones[step])\n",
    "            delta = rewards[step] + self.gamma * values[step + 1] * (1 - dones[step]) - values[step]\n",
    "            gae = delta + self.gamma * self.lam * (1 - dones[step]) * gae\n",
    "            advantages.append(gae)\n",
    "        advantages.reverse()\n",
    "        return advantages\n",
    "    \n",
    "    def process_gae_returns(self,rewards, values, dones):\n",
    "        with torch.no_grad():\n",
    "            returns = []\n",
    "            # G = 0\n",
    "            # # Compute returns for each timestep\n",
    "            # for reward, done in zip(reversed(rewards), reversed(dones)):\n",
    "            #     G = reward + self.gamma * G * (1 - done)\n",
    "            #     returns.append(G)\n",
    "            # returns.reverse()\n",
    "            \n",
    "            # Compute advantages using GAE\n",
    "            advantages = self.compute_gae(rewards, values, dones)\n",
    "            advantages = torch.FloatTensor(advantages).to(self.device)\n",
    "            # Normalize advantages for training stability\n",
    "            # advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "            \n",
    "            # Prepare tensors\n",
    "            # returns = torch.FloatTensor(returns).to(self.device)\n",
    "            returns = advantages + torch.FloatTensor(values).to(self.device)\n",
    "\n",
    "        return returns,advantages\n",
    "    def process_gae_returns0(self,rewards, values, dones):\n",
    "        with torch.no_grad():\n",
    "            returns = []\n",
    "            G = 0\n",
    "            # Compute returns for each timestep\n",
    "            for reward, done in zip(reversed(rewards), reversed(dones)):\n",
    "                G = reward + self.gamma * G * (1 - done)\n",
    "                returns.append(G)\n",
    "            returns.reverse()\n",
    "            \n",
    "            # Compute advantages using GAE\n",
    "            advantages = self.compute_gae(rewards, values, dones)\n",
    "            advantages = torch.FloatTensor(advantages).to(self.device)\n",
    "            # Normalize advantages for training stability\n",
    "            # advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "            \n",
    "            # Prepare tensors\n",
    "            returns = torch.FloatTensor(returns).to(self.device)\n",
    "            # returns = advantages + torch.FloatTensor(values).to(self.device)\n",
    "\n",
    "        return returns,advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards = [0,0,0,0,1,0,0,-1,0,0,0,1,0,0,1,0]\n",
    "dones =   [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "next_done = 1\n",
    "\n",
    "# values =  np.random.rand(len(rewards))\n",
    "values = [0.47495744, 0.04955575, 0.88012086, 0.81747744, 0.47915682,\n",
    "       0.75750459, 0.11583911, 0.42138228, 0.77161771, 0.4410075 ,\n",
    "       0.72813253, 0.42859623, 0.50210429, 0.47359902, 0.91350348,\n",
    "       0.97346051]\n",
    "\n",
    "next_value = 0 # can be any, as next_done is 1\n",
    "len(dones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.7926, 1.8107, 1.8290, 1.8475, 1.8661, 0.8749, 0.8837, 0.8927, 1.9118,\n",
       "         1.9311, 1.9506, 1.9703, 0.9801, 0.9900, 1.0000, 0.0000]),\n",
       " tensor([ 0.8716,  1.3796,  0.5931,  0.7059,  1.1154, -0.1652,  0.5078,  0.2195,\n",
       "          0.9325,  1.3477,  1.1354,  1.5303,  0.4910,  0.5574,  0.1347, -0.9735]))"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returns,advantages = ppo_agent().process_gae_returns0(rewards, values, dones[1:]+[1])\n",
    "(returns, advantages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.3465, 1.4291, 1.4732, 1.5234, 1.5945, 0.5923, 0.6236, 0.6409, 1.7041,\n",
       "         1.7887, 1.8636, 1.9589, 0.9931, 1.0310, 1.0482, 0.0000]),\n",
       " tensor([ 0.8716,  1.3796,  0.5931,  0.7059,  1.1154, -0.1652,  0.5078,  0.2195,\n",
       "          0.9325,  1.3477,  1.1354,  1.5303,  0.4910,  0.5574,  0.1347, -0.9735]))"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our ppo-test collected is actually next done\n",
    "returns,advantages = ppo_agent().process_gae_returns(rewards, values, dones[1:]+[1])\n",
    "(returns, advantages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.47495744, 0.04955575, 0.88012086, 0.81747744, 0.47915682, 0.75750459, 0.11583911, 0.42138228, 0.77161771, 0.4410075, 0.72813253, 0.42859623, 0.50210429, 0.47359902, 0.91350348, 0.97346051, 0]\n",
      "15 tensor(0.) 0 0.0\n",
      "14 tensor(1.) 0.97346051 1.0\n",
      "13 tensor(0.) 0.91350348 1.0\n",
      "12 tensor(0.) 0.47359902 1.0\n",
      "11 tensor(1.) 0.50210429 1.0\n",
      "10 tensor(0.) 0.42859623 1.0\n",
      "9 tensor(0.) 0.72813253 1.0\n",
      "8 tensor(0.) 0.4410075 1.0\n",
      "7 tensor(-1.) 0.77161771 1.0\n",
      "6 tensor(0.) 0.42138228 1.0\n",
      "5 tensor(0.) 0.11583911 1.0\n",
      "4 tensor(1.) 0.75750459 1.0\n",
      "3 tensor(0.) 0.47915682 1.0\n",
      "2 tensor(0.) 0.81747744 1.0\n",
      "1 tensor(0.) 0.88012086 1.0\n",
      "0 tensor(0.) 0.04955575 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([1.3465, 1.4291, 1.4732, 1.5234, 1.5945, 0.5923, 0.6236, 0.6409, 1.7041,\n",
       "         1.7887, 1.8636, 1.9589, 0.9931, 1.0310, 1.0482, 0.0000]),\n",
       " tensor([ 0.8716,  1.3796,  0.5931,  0.7059,  1.1154, -0.1652,  0.5078,  0.2195,\n",
       "          0.9325,  1.3477,  1.1354,  1.5303,  0.4910,  0.5574,  0.1347, -0.9735]))"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CleanRL\n",
    "class args:\n",
    "    num_steps = len(rewards)\n",
    "    gamma = 0.99\n",
    "    gae_lambda = 0.95\n",
    "    \n",
    "def cleanrl_gae_returns(rewards, values, dones, args, device='cpu'):\n",
    "    with torch.no_grad():\n",
    "        print(values)\n",
    "        next_value = values[-1]\n",
    "        advantages = torch.zeros_like(rewards).to(device)\n",
    "        lastgaelam = 0\n",
    "        for t in reversed(range(args.num_steps)):\n",
    "            if t == args.num_steps - 1:\n",
    "                nextnonterminal = 1.0 - dones[-1]\n",
    "                nextvalues = next_value\n",
    "            else:\n",
    "                nextnonterminal = 1.0 - dones[t + 1]\n",
    "                nextvalues = values[t + 1]\n",
    "            print(t, rewards[t], nextvalues, nextnonterminal)\n",
    "            delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]\n",
    "            advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam\n",
    "        returns = advantages + torch.FloatTensor(values[:-1]).to(device)\n",
    "    return returns, advantages\n",
    "\n",
    "cleanrl_gae_returns(torch.FloatTensor(rewards), values+[0], dones+[1], args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
