{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"ALE/Kaboom-v5\", render_mode='rgb_array')\n",
    "from nn_model6 import DQNEfficientNet\n",
    "model = DQNEfficientNet(env.action_space.n)\n",
    "\n",
    "wei = torch.load(\"trains/model_480.pth\", weights_only=True)\n",
    "model.load_state_dict(wei)\n",
    "for param in model.parameters():\n",
    "    print(f'number of parameters:{param.numel(): 12d} {param.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, axs =  plt.subplots(4,8,figsize=(10, 5))\n",
    "for c in range(32):\n",
    "    with torch.no_grad():\n",
    "        max = model.features[0].weight[c].max().item()\n",
    "        min = model.features[0].weight[c].min().item()\n",
    "        img_norm = (model.features[0].weight[c].permute(1,2,0).numpy()-min)/(max - min)\n",
    "        # print(max, min)\n",
    "        axs[c//8][c%8].imshow(img_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "state,_ = env.reset()\n",
    "def obs2stateTensor(obs, show=False):\n",
    "    # obs = obs[:, 420:1500] # choose area for model input\n",
    "    # state = cv2.resize(obs, (224, 224))\n",
    "    state = obs\n",
    "    if(show): \n",
    "        cv2.imshow('model input', cv2.cvtColor(state, cv2.COLOR_RGB2BGR))\n",
    "        cv2.waitKey(1)\n",
    "    state = torch.FloatTensor(state)\n",
    "    state = state.permute(2, 0, 1).contiguous()  # (H, W, C) -> C, H, W\n",
    "    state = (state - state.mean())/state.std() # normalize\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_axs = plt.subplot()\n",
    "fig, axs =  plt.subplots(8,8,figsize=(10, 10))\n",
    "with torch.no_grad():\n",
    "    state, reward, done,trancated, info = env.step(env.action_space.sample())\n",
    "    img_axs.imshow(state)\n",
    "    state = obs2stateTensor(state, show=False)\n",
    "    afterCNN = model.features[0:8](state)\n",
    "    print(afterCNN.shape)\n",
    "    for c in range(16):\n",
    "        axs[c//8][c%8].imshow(afterCNN[c], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state,_ = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_axs = plt.subplot()\n",
    "fig, axs =  plt.subplots(1,2,figsize=(16, 5), gridspec_kw={'width_ratios': [1, 4]})\n",
    "\n",
    "with torch.no_grad():\n",
    "    state, reward, done,trancated, info = env.step(env.action_space.sample())\n",
    "    axs[0].imshow(state)\n",
    "    state = obs2stateTensor(state, show=False)\n",
    "    afterCNN = model.features[0:6](state)\n",
    "    #normalize each img\n",
    "    max = afterCNN.amax(dim= (-1,-2), keepdim=True)\n",
    "    min = afterCNN.amin(dim= (-1,-2), keepdim=True)\n",
    "    afterCNN = (afterCNN - min)/(max - min + 1e-8)    \n",
    "    afterCNN = torch.nn.functional.pad(afterCNN, (0, 1, 0, 1), mode='constant',value=1) # pad to add border\n",
    "    print(afterCNN.shape)\n",
    "    img_w = afterCNN.shape[-1]\n",
    "    img_h = afterCNN.shape[-2]\n",
    "    img_c = afterCNN.shape[-3]\n",
    "    afterCNN = afterCNN.reshape(-1,8,img_h,img_w).permute(0,2,1,3).reshape(-1,8*img_w)\n",
    "    print(afterCNN.shape)\n",
    "    axs[1].imshow(afterCNN, cmap='gray')\n",
    "    # print(cv2.resize((afterCNN.unsqueeze(-1)*torch.ones(3)).numpy(), (420, afterCNN.shape[-1]*320/afterCNN.shape[-2])).shape)\n",
    "    \n",
    "    # for c in range(16):\n",
    "    #     axs[c//8][c%8].imshow(afterCNN[c], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play the game use the model and display interal CNN features\n",
    "import time\n",
    "import numpy as np\n",
    "cnn_layer = 8\n",
    "num_img_horiz = 2\n",
    "obs,_ = env.reset()\n",
    "for i in range(10000000):\n",
    "    with torch.no_grad():\n",
    "        state = obs2stateTensor(obs, show=False)\n",
    "        q_values = model(state.unsqueeze(0))\n",
    "        action = q_values.argmax().item() \n",
    "        obs, reward, done,trancated, info = env.step(action)\n",
    "        if(done):\n",
    "            obs,_ = env.reset()\n",
    "            print('Started new game')\n",
    "            continue\n",
    "        afterCNN = model.features[0:cnn_layer](state)\n",
    "        #normalize each img\n",
    "        # max = afterCNN.amax(dim= (-1,-2), keepdim=True)\n",
    "        # min = afterCNN.amin(dim= (-1,-2), keepdim=True)\n",
    "        max = afterCNN.max()\n",
    "        min = afterCNN.min()\n",
    "        afterCNN = (afterCNN - min)/(max - min + 1e-8)    \n",
    "        afterCNN = torch.nn.functional.pad(afterCNN, (0, 1, 0, 1), mode='constant',value=1) # pad to add border\n",
    "        # print(afterCNN.shape)\n",
    "        img_w = afterCNN.shape[-1]\n",
    "        img_h = afterCNN.shape[-2]\n",
    "        img_c = afterCNN.shape[-3]\n",
    "        afterCNN = afterCNN.reshape(-1,num_img_horiz,img_h,img_w).permute(0,2,1,3).reshape(-1,num_img_horiz*img_w)\n",
    "        # print(afterCNN.shape)\n",
    "        # print((afterCNN).max())\n",
    "        state = cv2.cvtColor(obs.astype(np.float32)/200, cv2.COLOR_RGB2BGR)\n",
    "        cv_img = cv2.hconcat([cv2.resize(state,(640,840), interpolation=cv2.INTER_AREA), \n",
    "                            cv2.resize((afterCNN.unsqueeze(-1)*torch.ones(3)).numpy(), (int(afterCNN.shape[-1]*840/afterCNN.shape[-2]), 840)\n",
    "                                       ,interpolation=cv2.INTER_AREA)])\n",
    "        cv2.imshow('Game Analysis', cv_img)\n",
    "        cv2.waitKey(1)\n",
    "        time.sleep(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "theta = torch.tensor(0., requires_grad=True)\n",
    "X = torch.tensor([0,1,2], requires_grad=False)\n",
    "with torch.no_grad():\n",
    "    probs_dist = torch.distributions.Categorical((X+theta)/(3*theta+3))\n",
    "    X_samples = probs_dist.sample([10000])\n",
    "    # probs_dist.log_prob(torch.tensor([0,1,2]))\n",
    "\n",
    "X_samples = probs_dist.sample([1000000]) # use monte carlo to estimate expectation\n",
    "def p(x, theta):\n",
    "    return (x+theta)/(3*theta+3) #p(x|theta)\n",
    "# print(X_samples)\n",
    "dd = torch.log(p(X_samples, theta))*X_samples #ln(p(x|theta))*x\n",
    "(dd.mean()).backward() # backward to get the gradient of ln(p(x|theta))*x, mean is to estimate expectation\n",
    "print(theta.grad) # the gradient, should be -2/3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import frame_stack,AtariPreprocessing\n",
    "\n",
    "env = gym.make(\"ALE/Pong-v5\")  # Create an Atari Pong environment\n",
    "\n",
    "env = AtariPreprocessing(env, grayscale_obs = True,scale_obs=True)\n",
    "env = frame_stack.FrameStack(env, 4)\n",
    "# env = wrap_deepmind(env, frame_stack=True, scale=True)  # Apply Atari preprocessing\n",
    "\n",
    "\n",
    "# Now, the environment provides preprocessed observations\n",
    "# observation = env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "observation = env.step(env.action_space.sample())\n",
    "plt.imshow(observation[0], cmap='gray')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
